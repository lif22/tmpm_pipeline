{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba656de",
   "metadata": {},
   "source": [
    "# Executable for conducting the LDA parameter study\n",
    "This is the executable for conducting the parameter study for the Latent Dirichlet Allocation as outlined in the thesis in 6.4.2\n",
    "Run according cells to obtain results for each case, rename filename in last cell and run to store results as pickle in \"out/parameter_study/filename.pkl\".\n",
    "Use \"out/parameter_study/plot.ipynb\" to plot results of parameter study.\n",
    "\n",
    "**Author: Florian Lietz**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d109f6",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "from nltk.chunk import ne_chunk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "from stages.utils.utils import parseArgs, DataCleaner\n",
    "from stages.TM.textmining import Message, Case, CasesList, lemmatize, gen_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d07d0",
   "metadata": {},
   "source": [
    "## Import, preprocessing, case clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = path.join(\"resources\", \"dataset\", \"Mail_ApplicationDataset_-2.csv\")\n",
    "# import NLD file\n",
    "inputFile = pd.read_csv(infile, delimiter=\";\")\n",
    "cleaner = DataCleaner(\n",
    "    removeURLs=True,\n",
    "    removeMultWhitespace=True,\n",
    "    lowercasing=False,\n",
    "    dateFormat=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "cleaner.apply(inputFile)\n",
    "casesList = CasesList.groupCases(file=inputFile, maxDays=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0ae55",
   "metadata": {},
   "source": [
    "## PrettyPrint case list for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "casesList.prettyprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476d36a",
   "metadata": {},
   "source": [
    "## Text Mining - Preprocessing Lemmatization / Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "# gensim\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "from gensim.utils import pickle, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy\n",
    "import spacy\n",
    "# vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "corpora_msg = casesList.getCorpora()\n",
    "len(corpora_msg)\n",
    "\n",
    "# uncomment lines below depending on parameter study\n",
    "corpora_lemmatized = lemmatize(corpora_msg)\n",
    "corpora_prep = gen_words(corpora_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72498b6d",
   "metadata": {},
   "source": [
    "## Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_phrases = gensim.models.Phrases(\n",
    "    corpora_prep,\n",
    "    min_count=5,\n",
    "    threshold=20\n",
    ")\n",
    "trigram_phrases = gensim.models.Phrases(\n",
    "    bigrams_phrases[corpora_prep],\n",
    "    threshold=20\n",
    ")\n",
    "bigram = gensim.models.phrases.Phraser(bigrams_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "def make_trigrams(texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(corpora_prep)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "# Train with dataset\n",
    "id2word = gensim.corpora.Dictionary(data_bigrams_trigrams)\n",
    "corp = [id2word.doc2bow(text) for text in data_bigrams_trigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954f947",
   "metadata": {},
   "source": [
    "## TF-IDF removal of low information value terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1134321",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(corpora_prep)\n",
    "corp = [id2word.doc2bow(text) for text in corpora_prep]\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(corp, id2word=id2word)\n",
    "low_value = 0.02 # threshold\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "for i in range(0,len(corp)):\n",
    "    bow = corp[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id,value in tfidf[bow]]\n",
    "    bow_ids = [id for id,value in bow]\n",
    "    low_value_words = [id for id,value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # words with tfidf score == 0\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corp[i] = new_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f3851",
   "metadata": {},
   "source": [
    "## LDA Model - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad852974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if id2word/corp has not generated before\n",
    "id2word = gensim.corpora.Dictionary(corpora_prep)\n",
    "corp = [id2word.doc2bow(text) for text in corpora_prep]\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "# Start of optimization operation\n",
    "\n",
    "num_pool = list(range(5,31))\n",
    "pool_scores = []\n",
    "for pool in num_pool:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corp, # corp\n",
    "        id2word=id2word,\n",
    "        num_topics=pool,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha=\"auto\"\n",
    "    )\n",
    "\n",
    "    # assign detected labels to every message\n",
    "    for case in casesList:\n",
    "        for message in case.messages:\n",
    "            cor = message.subject + \" \" + message.content\n",
    "            new_text_corpus = id2word.doc2bow(cor.split())\n",
    "            highestPercentageLabel = max(lda_model[new_text_corpus], key=lambda x:x[1])[0]\n",
    "            if (message.from_.split(\"@\")[1] == message.to.split(\"@\")[1]):\n",
    "                message.detectedLabel = 3\n",
    "            else:\n",
    "                message.detectedLabel = highestPercentageLabel\n",
    "\n",
    "    # get all train labels\n",
    "    labels = casesList.getTrainLabels()\n",
    "    # get amount of similar classifications for each label\n",
    "    quotas = []\n",
    "    print(f\"Stats for n_topics={pool}:\")\n",
    "    print(\"---------------------------\")\n",
    "    for label in labels:\n",
    "        # get all messages with label and check similar\n",
    "        msg = casesList.getMessagesByLabel(label)\n",
    "        detectedLabels = [m.detectedLabel for m in msg]\n",
    "        try:\n",
    "            max_ocurr = (max(sorted(set([i for i in detectedLabels if detectedLabels.count(i)>2]))))\n",
    "        except:\n",
    "            max_occur = 0\n",
    "        num = detectedLabels.count(max_ocurr)\n",
    "        rest = len(detectedLabels)-num\n",
    "        quota = num/(rest+num)\n",
    "        print(f\"Correctly identified messages for label {label}: {num}/{num+rest}={quota}\")\n",
    "        quotas.append([quota, len(detectedLabels)])\n",
    "    label_elcounts = [x[1] for x in quotas]\n",
    "    labelscores = [x[0] for x in quotas]\n",
    "    weighted = [a*b for a,b in zip(label_elcounts, labelscores)]\n",
    "    overall = sum(weighted)/sum(label_elcounts)\n",
    "    print(f\"Overall result: {overall}\")\n",
    "    pool_scores.append((pool, overall, quotas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c746d",
   "metadata": {},
   "source": [
    "## Save to pickle file\n",
    "**Change filename of .pkl file according to parameter study conducted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save Parameter Study values for later plotting (change filename accordingly)\n",
    "filename = \"lemma_simple_tfidf3.pkl\"\n",
    "p = path.join(\"out\", \"parameter_study\")\n",
    "\n",
    "with open(path.join(p, filename), \"wb\") as f:\n",
    "    pickle.dump(pool_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
